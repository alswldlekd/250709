import streamlit as st
import pandas as pd
import datetime
import time
import urllib.parse
from collections import Counter
import os
from utils_favorites import load_favorites, add_favorite, remove_favorite

# âœ… í•˜ë“œì½”ë”©ëœ í¬ë¡¬ë“œë¼ì´ë²„ ê²½ë¡œ
CHROMEDRIVER_PATH = r"C:\Users\ì¡°ë¯¼ì§€\Desktop\local ai\web\chromedriver-win64\chromedriver.exe"

# -----------------------------
# âœ… í¬ë¡¤ëŸ¬ í•¨ìˆ˜
# -----------------------------
def crawl_naver_cafe(SEARCH_KEYWORD, LAST_PAGE, DRIVER_PATH):
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from bs4 import BeautifulSoup

    ENCODED_KEYWORD = urllib.parse.quote(SEARCH_KEYWORD)
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    service = Service(executable_path=DRIVER_PATH)
    driver = webdriver.Chrome(service=service, options=chrome_options)

    results = []

    def parse_page(html, page_num):
        soup = BeautifulSoup(html, "html.parser")
        related_keywords = [btn.get_text(strip=True) for btn in soup.select('.aside_search_tag button')]
        related_keywords_str = ", ".join(related_keywords)
        for item in soup.select('.ArticleItem'):
            link_tag = item.select_one('a')
            if link_tag:
                link = link_tag['href']
                title_elem = link_tag.select_one('strong.title')
                text_elem = link_tag.select_one('p.text')
                title = title_elem.get_text(strip=True) if title_elem else ""
                snippet = text_elem.get_text(strip=True) if text_elem else ""
            else:
                link = title = snippet = ""
            date_elem = item.select_one('span.date')
            date = date_elem.get_text(strip=True) if date_elem else ""
            results.append({
                "í˜ì´ì§€": page_num,
                "ë‚ ì§œ": date,
                "ì œëª©": title,
                "ë§í¬": link,
                "ë³¸ë¬¸": snippet,
                "ì—°ê´€ê²€ìƒ‰ì–´": related_keywords_str
            })

    base_url = f"https://section.cafe.naver.com/ca-fe/home/search/articles?q={ENCODED_KEYWORD}"
    driver.get(base_url)
    time.sleep(5)
    current_block = 1
    for page in range(1, LAST_PAGE + 1):
        if page == 1:
            parse_page(driver.page_source, page)
            continue
        try:
            target_block = ((page - 1) // 10) + 1
            while current_block < target_block:
                next_btn = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, "button.type_next"))
                )
                driver.execute_script("arguments[0].click();", next_btn)
                time.sleep(3)
                current_block += 1
            btn_xpath = f"//button[@class='btn number' and normalize-space(text())='{page}']"
            page_btn = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.XPATH, btn_xpath))
            )
            driver.execute_script("arguments[0].click();", page_btn)
            time.sleep(4)
            parse_page(driver.page_source, page)
        except Exception as e:
            st.warning(f"[ì˜¤ë¥˜] {page}í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")

    driver.quit()
    return pd.DataFrame(results)

# -----------------------------
# âœ… ë¶„ì„ í•¨ìˆ˜
# -----------------------------
def clean_date(date_string, crawl_date=None):
    if crawl_date is None:
        crawl_date = datetime.date.today()
    if pd.isna(date_string):
        return str(crawl_date)
    date_string = str(date_string).strip()
    if any(keyword in date_string for keyword in ['ì „', 'ì‹œê°„', 'ë¶„', 'ì¼']):
        return str(crawl_date)
    return date_string

def standardize_date_format(date_string):
    if pd.isna(date_string):
        return date_string
    date_string = str(date_string).strip()
    if date_string.endswith('.'):
        date_string = date_string[:-1]
    return date_string.replace('.', '-')

def extract_top_keywords(df, top_n=5):
    from collections import Counter
    all_keywords = []
    for kw_list in df['ì—°ê´€ê²€ìƒ‰ì–´'].dropna():
        all_keywords.extend([kw.strip() for kw in kw_list.split(',') if kw.strip()])
    for text in df['ì œëª©'].dropna():
        all_keywords.extend(text.split())
    for text in df['ë³¸ë¬¸'].dropna():
        all_keywords.extend(text.split())
    counter = Counter(all_keywords)
    return [kw for kw, _ in counter.most_common(top_n)]

# âœ… ê°œì„ ëœ ì´ìƒê°ì§€ í•¨ìˆ˜
def detect_anomaly_dynamic(df, date_column='ë‚ ì§œ', recent_days=10, window=3):
    daily_counts = df[date_column].value_counts().sort_index()
    daily_counts.index = pd.to_datetime(daily_counts.index, errors='coerce')
    daily_counts = daily_counts.dropna().sort_index()

    # ìµœê·¼ Nì¼ë§Œ ìë¥´ê¸°
    if len(daily_counts) == 0:
        return [], None, daily_counts, None, None, None

    latest_date = daily_counts.index.max()
    cutoff_date = latest_date - pd.Timedelta(days=recent_days - 1)
    daily_counts_recent = daily_counts[daily_counts.index >= cutoff_date]

    if len(daily_counts_recent) == 0:
        return [], None, daily_counts, None, None, None

    # âœ… ì´ í¬ë¡¤ë§ ê°œìˆ˜ ëŒ€ë¹„ ìµœê·¼ Nì¼ ë¹„ì¤‘ ê³„ì‚°
    total_count = len(df)
    recent_dates_str = daily_counts_recent.index.strftime('%Y-%m-%d')
    recent_count = len(df[df[date_column].isin(recent_dates_str)])
    recent_ratio = recent_count / total_count if total_count > 0 else 0

    # âœ… ë¹„ì¤‘ ê¸°ë°˜ ë¯¼ê°ë„(k) ì„¤ì •
    if recent_ratio > 0.8:
        k = 1.0
        attention_factor = 1.2
    elif recent_ratio > 0.5:
        k = 1.2
        attention_factor = 1.25
    elif recent_ratio > 0.2:
        k = 1.5
        attention_factor = 1.3
    else:
        k = 2.0
        attention_factor = 1.5

    # âœ… ì„ê³„ì¹˜ ê³„ì‚°
    moving_avg = daily_counts_recent.rolling(window=window, min_periods=1).mean()
    moving_std = daily_counts_recent.rolling(window=window, min_periods=1).std().fillna(0)
    threshold_series = moving_avg + k * moving_std

    # âœ… ì´ìƒ ê°ì§€
    anomalies_idx = daily_counts_recent[daily_counts_recent > threshold_series].index
    anomalies = anomalies_idx.strftime("%Y-%m-%d").tolist()

    # âœ… ì˜¤ëŠ˜ ê¸°ì¤€ ê°ì§€
    today_str = str(datetime.date.today())
    today_count = daily_counts_recent.get(pd.to_datetime(today_str), 0)
    threshold_today = threshold_series.get(pd.to_datetime(today_str), None)

    return anomalies, threshold_today, daily_counts_recent, k, attention_factor, recent_ratio



#ê¸‰ë½ ê°ì§€ í•¨ìˆ˜
def detect_drop_alert(daily_counts_recent, drop_ratio=0.3):
    """
    ìµœê·¼ ë©°ì¹ ê°„ í”¼í¬ ëŒ€ë¹„ ì˜¤ëŠ˜ ê¸‰ë½ ì—¬ë¶€ë¥¼ ê°ì§€
    drop_ratio: ê¸‰ë½ ê²½ê³„ ë¹„ìœ¨ (default 30%)
    """
    if len(daily_counts_recent) < 3:
        return None, None

    today = pd.to_datetime(datetime.date.today())
    if today not in daily_counts_recent.index:
        return None, None

    today_count = daily_counts_recent.get(today, 0)
    peak_count = daily_counts_recent.drop(today, errors='ignore').max()

    if peak_count == 0:
        return None, None

    ratio = today_count / peak_count

    if ratio < drop_ratio:
        return ratio, peak_count
    else:
        return None, peak_count

#ì–´ì œ ëŒ€ë¹„ ì˜¤ëŠ˜
def detect_yesterday_change_alert(today_count, yesterday_count):
    if yesterday_count == 0:
        if today_count > 3:
            return "ê²½ê³ ", f"ì–´ì œ 0ê±´ â†’ ì˜¤ëŠ˜ {today_count}ê±´ ê¸‰ì¦"
        elif today_count > 0:
            return "ì£¼ì˜", f"ì–´ì œ 0ê±´ â†’ ì˜¤ëŠ˜ {today_count}ê±´ ì¦ê°€"
        else:
            return None, None

    change_ratio = (today_count - yesterday_count) / yesterday_count

    if change_ratio > 2.0:
        return "ê²½ê³ ", f"ì–´ì œ ëŒ€ë¹„ +{change_ratio*100:.0f}% ê¸‰ì¦"
    elif change_ratio > 1.0:
        return "ì£¼ì˜", f"ì–´ì œ ëŒ€ë¹„ +{change_ratio*100:.0f}% ì¦ê°€"
    elif change_ratio < -0.5:
        return "ê²½ê³ ", f"ì–´ì œ ëŒ€ë¹„ {change_ratio*100:.0f}% ê¸‰ê°"
    elif change_ratio < -0.25:
        return "ì£¼ì˜", f"ì–´ì œ ëŒ€ë¹„ {change_ratio*100:.0f}% ê°ì†Œ"
    else:
        return None, None


# -----------------------------
# âœ… Streamlit í˜ì´ì§€
# -----------------------------
st.set_page_config(page_title="ì´ìƒ ê°ì§€", layout="wide")
st.title("ğŸ“ˆ ë„¤ì´ë²„ ì¹´í˜ ì´ìƒ ê°ì§€ ë° ì‹œê°í™”")

# âœ… ë’¤ë¡œê°€ê¸° ë²„íŠ¼
st.page_link("", label="ğŸ  í™ˆìœ¼ë¡œ", icon="ğŸ ")

# âœ… ì¦ê²¨ì°¾ê¸° ê´€ë¦¬ ì‚¬ì´ë“œë°”
st.sidebar.header("â­ ì¦ê²¨ì°¾ê¸° ê´€ë¦¬")
favorites = load_favorites()

# âœ… ì¦ê²¨ì°¾ê¸° ì¶”ê°€
new_fav = st.sidebar.text_input("ì¦ê²¨ì°¾ê¸°ì— ì¶”ê°€í•  í‚¤ì›Œë“œ", "")
if st.sidebar.button("â­ ì¶”ê°€"):
    if new_fav.strip():
        favorites = add_favorite(new_fav.strip())
        st.sidebar.success(f"âœ… '{new_fav}' ì¶”ê°€ ì™„ë£Œ!")

# âœ… ì¦ê²¨ì°¾ê¸° ëª©ë¡
if favorites:
    st.sidebar.markdown("### âœ… ë‚´ ì¦ê²¨ì°¾ê¸°")
    for fav in favorites:
        col1, col2 = st.sidebar.columns([4, 1])
        col1.markdown(f"- {fav}")
        if col2.button("âŒ", key=f"del_{fav}"):
            favorites = remove_favorite(fav)
            st.sidebar.warning(f"âŒ '{fav}' ì œê±°ë¨")

st.header("ğŸ› ï¸ í¬ë¡¤ëŸ¬ ì„¤ì •")

# âœ… ì¦ê²¨ì°¾ê¸° ì„ íƒ
selected_fav = None
if favorites:
    selected_fav = st.selectbox("â­ ì¦ê²¨ì°¾ê¸° í‚¤ì›Œë“œ", favorites, placeholder="ì„ íƒí•˜ì§€ ì•ŠìŒ")

custom_keyword = st.text_input("âœ… ì§ì ‘ ì…ë ¥", "")

# âœ… ìµœì¢… í‚¤ì›Œë“œ ê²°ì •
final_keyword = None
if selected_fav and selected_fav.strip():
    final_keyword = selected_fav.strip()
elif custom_keyword.strip():
    final_keyword = custom_keyword.strip()

if final_keyword:
    st.info(f"ğŸ‘‰ **ìµœì¢… í‚¤ì›Œë“œ: {final_keyword}**")
else:
    st.warning("â— í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ì¦ê²¨ì°¾ê¸°ì—ì„œ ì„ íƒí•˜ì„¸ìš”.")


last_page = st.slider("âœ… í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜", 1, 50, 10)

start_crawl = st.button("ğŸš€ í¬ë¡¤ë§ ì‹œì‘")

if start_crawl:
    if not os.path.isfile(CHROMEDRIVER_PATH):
        st.error(f"âŒ í•˜ë“œì½”ë”©ëœ ChromeDriver ê²½ë¡œ ì˜¤ë¥˜!\n```\n{CHROMEDRIVER_PATH}\n```")
    else:
        with st.spinner(f"ğŸ” '{final_keyword}' í¬ë¡¤ë§ ì¤‘..."):
            df = crawl_naver_cafe(final_keyword, last_page, CHROMEDRIVER_PATH)

        st.success(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(df)}ê±´")

        # âœ… ë‚ ì§œ ì²˜ë¦¬
        df['ë‚ ì§œ'] = df['ë‚ ì§œ'].apply(clean_date).apply(standardize_date_format)

        # âœ… ê°œì„ ëœ ì´ìƒê°ì§€
        anomalies, threshold_today, daily_counts_recent, k, attention_factor, recent_ratio = detect_anomaly_dynamic(df, recent_days=10)



        st.subheader("âœ… ë‚ ì§œë³„ ê²Œì‹œê¸€ ìˆ˜")
        st.bar_chart(daily_counts_recent)

        st.subheader("âœ… ì˜¤ëŠ˜ / ì–´ì œ ë¹„êµ")
        today_str = str(datetime.date.today())
        yesterday_str = str(datetime.date.today() - datetime.timedelta(days=1))
        today_count = daily_counts_recent.get(pd.to_datetime(today_str), 0)
        yesterday_count = daily_counts_recent.get(pd.to_datetime(yesterday_str), 0)
        diff_count = today_count - yesterday_count

        col1, col2 = st.columns(2)
        with col1:
            st.metric(label=f"ì˜¤ëŠ˜ ({today_str})", value=f"{today_count} ê±´", delta=f"{diff_count:+} ê±´")
        with col2:
            st.metric(label=f"ì–´ì œ ({yesterday_str})", value=f"{yesterday_count} ê±´")

        if diff_count > 0:
            st.success(f"ğŸ“ˆ ì˜¤ëŠ˜ ì–´ì œë³´ë‹¤ {diff_count}ê±´ ì¦ê°€!")
        elif diff_count < 0:
            st.warning(f"ğŸ“‰ ì˜¤ëŠ˜ ì–´ì œë³´ë‹¤ {-diff_count}ê±´ ê°ì†Œ!")
        else:
            st.info("â– ë³€í™” ì—†ìŒ")

        st.subheader("âœ… ì–´ì œ ëŒ€ë¹„ ë³€í™”ìœ¨ ê²½ë³´")
        alert_level, alert_message = detect_yesterday_change_alert(today_count, yesterday_count)
        if alert_level == "ê²½ê³ ":
            st.error(f"â— {alert_message}")
        elif alert_level == "ì£¼ì˜":
            st.warning(f"âš ï¸ {alert_message}")
        else:
            st.success("âœ… ì–´ì œ ëŒ€ë¹„ í° ë³€í™” ì—†ìŒ")

        st.subheader("âœ… ì´ìƒê°ì§€ ê²°ê³¼")
        if threshold_today is not None:
            st.write(f"ì˜¤ëŠ˜ ê¸°ì¤€ ì„ê³„ì¹˜ (k={k:.1f}): {threshold_today:.2f}")
            st.write(f"ì£¼ì˜ ë‹¨ê³„ ê³„ìˆ˜ (Attention Factor): {attention_factor:.2f}")

            # âœ… recent_ratio ì¹œì ˆ ì¶œë ¥
            st.info(
                f"ğŸ“Š ìµœê·¼ 10ì¼ì¹˜ ë°ì´í„° ë¹„ì¤‘: {recent_ratio*100:.1f}%\n"
                f"(ì „ì²´ í¬ë¡¤ë§ {len(df)}ê±´ ì¤‘ ìµœê·¼ 10ì¼ {int(recent_ratio * len(df))}ê±´)"
            )

            if today_count <= threshold_today:
                st.success(f"âœ… ì˜¤ëŠ˜({today_str}) ì–‘í˜¸ - ê¸‰ì¦ ì—†ìŒ")
            elif today_count <= threshold_today * attention_factor:
                st.warning(f"âš ï¸ ì˜¤ëŠ˜({today_str}) ì£¼ì˜ - í‰ì†Œë³´ë‹¤ ë‹¤ì†Œ ì¦ê°€")
            else:
                st.error(f"â— ì˜¤ëŠ˜({today_str}) ê²½ê³  - ê¸‰ì¦ ê°ì§€!")
        else:
            st.warning("ì˜¤ëŠ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


# âœ… ê¸‰ë½ ê°ì§€ ê²°ê³¼
        st.subheader("âœ… ìµœê·¼ í”¼í¬ ëŒ€ë¹„ ê¸‰ë½ ê²½ë³´")
        drop_ratio, recent_peak = detect_drop_alert(daily_counts_recent)
        if drop_ratio is not None:
            st.error(f"â— ì˜¤ëŠ˜ ê±´ìˆ˜ê°€ ìµœê·¼ í”¼í¬ ({recent_peak}ê±´)ì˜ {drop_ratio*100:.1f}% ìˆ˜ì¤€ìœ¼ë¡œ ê¸‰ë½!")
        else:
            if recent_peak is not None:
                st.success(f"âœ… ê¸‰ë½ ê²½ë³´ ì—†ìŒ (ìµœê·¼ í”¼í¬ {recent_peak}ê±´ ê¸°ì¤€)")
            else:
                st.info("â– ê¸‰ë½ ê°ì§€ë¥¼ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì¼ì •í•©ë‹ˆë‹¤.")


        st.subheader("âœ… ì¸ê¸° í‚¤ì›Œë“œ Top 5")
        extracted_keywords = extract_top_keywords(df)
        if extracted_keywords:
            for kw in extracted_keywords:
                st.markdown(f"- {kw}")
        else:
            st.write("ì¶”ì¶œëœ í‚¤ì›Œë“œ ì—†ìŒ.")

        st.subheader("âœ… ë‚ ì§œë³„ ê±´ìˆ˜ ìƒìœ„ 10")
        st.dataframe(df['ë‚ ì§œ'].value_counts().head(10))

        with st.expander("âœ… ì „ì²´ ë°ì´í„° ë³´ê¸°"):
            st.dataframe(df)



